{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48209991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\wajiz.pk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#  taking word list from nltk\n",
    "# first we import nltk library \n",
    "import nltk\n",
    "# now the issues arrises every time we run our code\n",
    "# can be different from previous one. \n",
    "# for that reason, we are using pickle library. \n",
    "import pickle\n",
    "# import Lemmatizer \n",
    "# reduce word to its simple form. \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#importing stopwords from nltk\n",
    "from nltk.corpus import stopwords\n",
    "# downloading words from nltk\n",
    "nltk.download('words')\n",
    "# first it check either file containing word exists or not. \n",
    "try:\n",
    "    # if yes, it exists then open file in read mode. \n",
    "    with open('word_list.pkl', 'rb') as file:\n",
    "        # and assign it to word list.\n",
    "        # pickle.load==> recontruct data in original form\n",
    "        word_list = pickle.load(file)\n",
    " # if not, then fetch english words from nltk        \n",
    "except FileNotFoundError:\n",
    "    # If the file doesn't exist, download the words and save them\n",
    "    word_list = nltk.corpus.words.words()\n",
    "    with open('word_list.pkl', 'wb') as file:\n",
    "        # pickle.dump==> store in binary form. \n",
    "        pickle.dump(word_list, file) \n",
    "\n",
    "# check the lebgth of word list \n",
    "length_of_wordlist=len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b2af845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202480"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now the length of word list is [236736] stored in var length_of_wordlist\n",
    "# by using function we assign unique wordId to word. \n",
    "\n",
    "def WordID(word):\n",
    "    #build in method to assign unique ids to words. \n",
    "    wordaddress=word_list.index(word) \n",
    "    # fetch word and return its id. \n",
    "    return wordaddress\n",
    "# function calling ==> word to wordId. \n",
    "WordID(\"thought\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1890e6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Getting the list of stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# length of stop words \n",
    "length_of_stopWords=len(stop_words)\n",
    "# we are now seperating the words from stop words \n",
    "# and storing them to list non_stop_words. \n",
    "non_stop_words=list(set(word_list)-stop_words)\n",
    "# just checking the length of non_stop_words. \n",
    "lenght_of_non_stop_words=len(non_stop_words)\n",
    "# now we merge the non_stop_word to stop_word \n",
    "# we did this to just seperate stop_words and non_stop_words\n",
    "sorted_words=list(stop_words).append(non_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a462ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jumping'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting the word to its simple form.\n",
    "# e.g jumping, jumped ==> jump \n",
    "def simplifyWord(word):\n",
    "    simple_word = WordNetLemmatizer()\n",
    "    Simplified_word = simple_word.lemmatize(word)\n",
    "    return Simplified_word\n",
    "simplifyWord(\"jumping\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e52aded",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     simplified_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc])\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m simplified_word\n\u001b[1;32m---> 15\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43msimplifyWord\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjumping\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36msimplifyWord\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimplifyWord\u001b[39m(word):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Load the English language model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Process the word using SpaCy\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     doc \u001b[38;5;241m=\u001b[39m nlp(word)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "def simplifyWord(word):\n",
    "    # Load the English language model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    # Process the word using SpaCy\n",
    "    doc = nlp(word)\n",
    "    \n",
    "    # Lemmatize the word\n",
    "    simplified_word = ' '.join([token.lemma_ for token in doc])\n",
    "    \n",
    "    return simplified_word\n",
    "\n",
    "result = simplifyWord(\"jumping\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3858d147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brotherhood\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def simplifyWord(word):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_word = stemmer.stem(word)\n",
    "    return stemmed_word\n",
    "\n",
    "result = simplifyWord(\"brotherhood\")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74549753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
